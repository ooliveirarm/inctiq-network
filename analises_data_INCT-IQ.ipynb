{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38c46ac",
   "metadata": {},
   "source": [
    "# Carregando bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a08e6e",
   "metadata": {},
   "source": [
    "Bibbliotecas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2d0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9abc71",
   "metadata": {},
   "source": [
    "Bibliotecas para análises de redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa947064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692e10b",
   "metadata": {},
   "source": [
    "Bibliotecas para plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804b4f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.unicode in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 112 ('text.latex.unicode : False # use \"ucs\" and \"inputenc\" LaTeX packages for handling')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key text.latex.preview in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 125 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 157 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 420 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 423 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key pgf.debug in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 444 ('pgf.debug           : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 475 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 476 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 493 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 504 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file C:\\Users\\teedo\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 506 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#nuvem de palavras\n",
    "import stylecloud as sc\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c90aa0",
   "metadata": {},
   "source": [
    "# Importando os dados da tabela \"INCT_IQ_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754eba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(name):\n",
    "    aux_n = unicodedata.normalize('NFD', name)\n",
    "    aux_n = aux_n.encode('ascii', 'ignore')\n",
    "    aux_n = aux_n.decode(\"utf-8\")\n",
    "    return aux_n\n",
    "\n",
    "bad = ['.','-']\n",
    "def lev(v):\n",
    "    v = v.replace('-',' ')\n",
    "    v = v.replace('.','')\n",
    "    v = v.replace(',','')\n",
    "    v = v.upper()\n",
    "    v1 = ' '.join(v.split())\n",
    "    return v1\n",
    "\n",
    "arq = \"INCT_IQ_data.xlsx\"\n",
    "\n",
    "#variável com os dados dos pesquisadores: cidade, centro e artigos\n",
    "df = pd.read_excel(arq, sheet_name=\"dados gerais pesquisadores A-Z\", \n",
    "                   usecols=[0, 1, 2, 3, 4], names=['author', 'uf', 'city', 'center', 'papers'])\n",
    "#limpeza da tabela df\n",
    "df = df.drop(0)\n",
    "df = df.drop(122)\n",
    "\n",
    "keys = ['author', 'uf', 'city', 'center']\n",
    "for key in keys:\n",
    "    df[key] = df[key].map(str)\n",
    "keys = ['papers']\n",
    "for key in keys:\n",
    "    df[key] = df[key].astype(float)\n",
    "    \n",
    "#variável com os dados dos pesquisadores: títulos, revistas e ano\n",
    "au_names = ['title', 'revista', 'year']\n",
    "au_names.extend(['authors' + '%d' % k for k in list(np.arange(0, 7, 1))])\n",
    "df0 = pd.read_excel(arq, sheet_name=\"Art.2016-2022 com revista e ano\", \n",
    "                   usecols=list(np.arange(0, 10, 1)), names=au_names)\n",
    "\n",
    "#limpeza da tabela df0\n",
    "keys = list(df0.keys())\n",
    "for key in keys:\n",
    "    df0[key] = df0[key].map(str)\n",
    "keys = ['year']\n",
    "for key in keys:\n",
    "    df0[key] = df0[key].astype(float)\n",
    "    \n",
    "#juntar os nomes dos mesmos autores\n",
    "df0[\"authors\"] = df0[\"authors0\"] + '|' + df0[\"authors1\"] + '|' + df0[\"authors2\"] + '|' + df0[\"authors3\"] + '|' + df0[\"authors4\"] + '|' + df0[\"authors5\"] + '|' + df0[\"authors6\"]\n",
    "\n",
    "#variável com a abreviação dos nomes dos pesquisadores\n",
    "names = ['Nome', 'Abrev']\n",
    "df2 = pd.read_excel(arq, sheet_name=\"Correspondencia dos nomes\", usecols=[0, 1], names = names)\n",
    "\n",
    "#transformando todas as informações em strings\n",
    "keys = list(df2.keys())\n",
    "for key in keys:\n",
    "    df2[key] = df2[key].map(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b6557",
   "metadata": {},
   "source": [
    "# Nuvem de palavras de todos os títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b535fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all = \" \".join(review for review in df0.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f918a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word = WordCloud().process_text(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1911c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8474"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([count_word[k] for k in count_word]) #total de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170395e",
   "metadata": {},
   "source": [
    "Top $10$ palavras que mais se repetem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdaf8c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum \t 300\n",
      "entanglement \t 69\n",
      "de \t 65\n",
      "state \t 63\n",
      "system \t 62\n",
      "effect \t 44\n",
      "Experimental \t 44\n",
      "optical \t 43\n",
      "using \t 42\n",
      "dynamic \t 41\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(count_word, key=count_word.get, reverse=True)[:10]:\n",
    "    print(k, '\\t', count_word[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb4e612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('titles-papers.txt','w',encoding='utf-8')\n",
    "f.write('%s\\n' % text_all)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc12a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"de\", \"two\", \"using\", \"via\", \"para\", \"work\", \"s\", \"e\", \"da\"])\n",
    "\n",
    "#número máximo de palavras\n",
    "max_words = 80\n",
    "wc=sc.gen_stylecloud(\n",
    "    file_path='titles-papers.txt', custom_stopwords=stopwords, icon_name = 'fas fa-square',\n",
    "    max_words=max_words, size=800, max_font_size = 150, output_name='figuras/nuvem-all.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707dcf6",
   "metadata": {},
   "source": [
    "# Número de artigos publicados por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d042cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearsl = [int(k) for k in list(df0.year)]\n",
    "nyears = dict(collections.Counter(sorted(yearsl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09d15ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2016: 184, 2017: 169, 2018: 172, 2019: 185, 2020: 215, 2021: 175, 2022: 56}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd6c77",
   "metadata": {},
   "source": [
    "# número de artigos publicados em revista específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d9656c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = [k.lower() for k in list(df0.revista)] #lower porque tem a mesma revista escrita de formas diferentes\n",
    "count_revs = dict(collections.Counter(sorted(revs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191e5ff",
   "metadata": {},
   "source": [
    "Top $10$ revistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dbcec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physical review a \t 267\n",
      "physical review letters \t 62\n",
      "physical review b \t 44\n",
      "quantum information processing \t 42\n",
      "physical review e \t 35\n",
      "scientific reports \t 26\n",
      "physical review research \t 23\n",
      "revista brasileira de ensino de física \t 22\n",
      "optics letters \t 17\n",
      "physics letters a \t 16\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(count_revs, key=count_revs.get, reverse=True)[:10]: #ver todos os nomes\n",
    "    print(k, '\\t', count_revs[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f45c96",
   "metadata": {},
   "source": [
    "# Número de pesquisadores por artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad95addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 892, 2: 166, 3: 64, 4: 15, 5: 16, 6: 2, 7: 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = [linha.replace(\"'\",\"\").split(\"|\") for linha in df0.authors]\n",
    "#remore nan from the list\n",
    "n2 = []\n",
    "for n in n1.copy():\n",
    "    cleanedList = [x for x in n if str(x) != 'nan']\n",
    "    n2.append(cleanedList)\n",
    "n1 = n2.copy()\n",
    "\n",
    "n_au = dict(collections.Counter(sorted([len(k) for k in n1])))\n",
    "n_au"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b83d",
   "metadata": {},
   "source": [
    "# Número de artigos publicados pelos pesquisadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbbb85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de pesquisadoras\n",
    "woman1 = ['Aline Medina dos Santos', 'Bárbara Lopes Amaral', 'Belita Koiller', \n",
    "            'Ginette Jalbert de Castro Faria', 'Halyne Silva Borges', 'Liliana Sanz de la Torre', \n",
    "           'Malena Osório Hor-Meyll', 'Maria Carolina de Oliveira Aguiar', 'Nadja Kolb Bernardes', \n",
    "           'Paula Borges Monteiro', 'Thereza Cristina de Lacerda Paiva']\n",
    "\n",
    "woman = ['MEDINA ALINE', 'AMARAL BARBARA', 'KOILLER BELITA', 'JALBERT GINETTE', 'AGUIAR M C O', 'BERNARDES N K',\n",
    "        'MONTEIRO P B', 'PAIVA T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "948c0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_autor = {i:int(j) for i,j in zip(list(df.author), list(df.papers))}\n",
    "number_aut = dict(collections.Counter(sorted(prod_autor.values())))\n",
    "\n",
    "#número de artigos publicados por mulheres\n",
    "womanp = sum([prod_autor[k] for k in prod_autor if k in woman1])\n",
    "#número de artigos publicados por homens\n",
    "manp = sum([prod_autor[k] for k in prod_autor]) - womanp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc828fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 5, 2: 3, 3: 4, 4: 8, 5: 3, 6: 5, 7: 8, 8: 7, 9: 10, 10: 8, 11: 5, 12: 9, 13: 4, 14: 3, 15: 2, 16: 1, 17: 5, 18: 3, 19: 3, 20: 3, 21: 4, 22: 3, 23: 2, 24: 2, 26: 1, 29: 1, 32: 2, 37: 1, 38: 1, 47: 1, 49: 2, 57: 1}\n"
     ]
    }
   ],
   "source": [
    "print(number_aut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389d285",
   "metadata": {},
   "source": [
    "# Número de pesquisadores, e publicações, em cada cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38a792f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. dict com o nome do pesquisador e sua cidade\n",
    "dcity = {i:j for i,j in zip(list(df.author), list(df.city))}\n",
    "#2. dict com o nome do pesquisador e centro que atua\n",
    "dcenter = {i:j for i,j in zip(list(df.author), list(df.center))}\n",
    "#3. dict com o nome do pesquisador e quantidade de artigos\n",
    "dpapers = {i:j for i,j in zip(list(df.author), list(df.papers))}\n",
    "\n",
    "#número de pesquisadores por cidade\n",
    "fcity = dict(collections.Counter(sorted(dcity.values())))\n",
    "#número de artigos por cidade\n",
    "\n",
    "#número de pesquisadores por centro\n",
    "fcenter = dict(collections.Counter(sorted(dcenter.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "572f30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arapiraca': 11.0, 'Araras': 7.0, 'Baurú': 13.0, 'Belo Horizonte': 141.0, 'Campinas': 115.0, 'Cornélio Procópio': 9.0, 'Cuiabá': 20.0, 'Curitiba': 23.0, 'Florianópolis': 74.0, 'Goiânia': 101.0, 'Itapeva': 7.0, 'Juiz de Fora': 4.0, 'Maceió': 21.0, 'Natal': 49.0, 'Niteroi': 161.0, 'Patrocínio': 6.0, 'Petrópolis': 4.0, 'Ponta Grossa': 26.0, 'Recife': 50.0, 'Rio de Janeiro': 293.0, 'Santa Maria': 26.0, 'Santo André': 47.0, 'Sorocaba': 2.0, 'São Carlos': 127.0, 'São Paulo': 116.0, 'Uberlândia': 11.0, 'Vitória': 57.0, 'Viçosa': 3.0, 'Volta Redonda': 57.0}\n"
     ]
    }
   ],
   "source": [
    "#número total de publicacções em cada cidade\n",
    "pub_total = {}\n",
    "for k in list(fcity.keys()):\n",
    "    pub_total[k] = 0\n",
    "for k in list(dpapers.keys()):\n",
    "    if k in list(dcity.keys()):\n",
    "        pub_total[dcity[k]] = pub_total[dcity[k]] + dpapers[k]\n",
    "print(pub_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a05532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CBPF': 94.0, 'CEFET-RJ': 4.0, 'IFRJ - Volta Redonda': 20.0, 'IFSC': 1.0, 'IFTM': 6.0, 'IIF': 49.0, 'UEPG': 26.0, 'UFABC': 47.0, 'UFAL': 32.0, 'UFES': 57.0, 'UFF': 198.0, 'UFG': 101.0, 'UFJF': 4.0, 'UFMG': 141.0, 'UFMT': 20.0, 'UFPE': 44.0, 'UFPR': 23.0, 'UFRJ': 156.0, 'UFRJ/CCMN': 43.0, 'UFRPE': 6.0, 'UFSC': 73.0, 'UFSCar': 47.0, 'UFSCar/CCA': 7.0, 'UFSCar/CCTS': 2.0, 'UFSM/CCNE': 26.0, 'UFU': 11.0, 'UFV': 3.0, 'UNESP': 10.0, 'UNESP - Bauru': 13.0, 'UNESP-Itapeva': 7.0, 'UNICAMP': 115.0, 'USP': 116.0, 'USP/IFSC': 70.0, 'UTFPR': 9.0}\n"
     ]
    }
   ],
   "source": [
    "#número total de publicacções dos centros/universidades\n",
    "pub_total_center = {}\n",
    "for k in list(fcenter.keys()):\n",
    "    pub_total_center[k] = 0\n",
    "for k in list(dpapers.keys()):\n",
    "    if k in list(dcenter.keys()):\n",
    "        pub_total_center[dcenter[k]] = pub_total_center[dcenter[k]] + dpapers[k]\n",
    "print(pub_total_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b9ae1",
   "metadata": {},
   "source": [
    "# Criação da rede de colaboração dos pesquisadores do INCT-IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6941b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict com as colaborações (relações) dos artigos\n",
    "#######################################################################\n",
    "############ CREDITOS: SAMURAÍ GOMES DE AGUIAR BRITO ##################\n",
    "#######################################################################\n",
    "net1 = {}\n",
    "for l in range(len(n1)):\n",
    "    aux = n1[l]\n",
    "    if len(aux) == 1: #Caso em que não ocorre a colaboração em um artigo\n",
    "        for i in aux:\n",
    "            i = dec(i)\n",
    "            i = lev(i)\n",
    "            i = i.lstrip()\n",
    "            if re.sub('[.-]','', i).replace(' ','').isalpha() and len(i.split())>1:\n",
    "                net1.setdefault(i.strip('71!/#&*$?+$'), {})\n",
    "    else:\n",
    "        for i in aux:\n",
    "            i = dec(i)\n",
    "            i = lev(i)\n",
    "            i = i.lstrip()\n",
    "            for j in aux:\n",
    "                j = dec(j)\n",
    "                j = lev(j)\n",
    "                j = j.lstrip()\n",
    "                if i!=j and re.sub('[.-]','', i).replace(' ','').isalpha() and re.sub('[.-]','', j).replace(' ','').isalpha():\n",
    "                    net1.setdefault(i.strip('71!/#&*$?+$'),{})[j.strip('71!/#&*$?+$')] = 0\n",
    "                    net1.setdefault(j.strip('71!/#&*$?+$'),{})[i.strip('71!/#&*$?+$')] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f31351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de artigos:                            1156\n",
      "Número total de pesquisadores:                      120\n"
     ]
    }
   ],
   "source": [
    "v_names_all = sorted(list(dict.fromkeys(net1).keys()))\n",
    "print('Número total de artigos:                            %d' % len(df0))\n",
    "print('Número total de pesquisadores:                      %d' % len(v_names_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85448de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "############ CREDITOS: SAMURAÍ GOMES DE AGUIAR BRITO ##################\n",
    "#######################################################################\n",
    "\n",
    "def network(net):\n",
    "    \n",
    "    list_name = sorted(list(net.keys()))\n",
    "    vnum = {v:i for i,v in enumerate(list_name)}\n",
    "    numv = {i:v for i,v in enumerate(list_name)}\n",
    "    \n",
    "    N = len(list_name)\n",
    "\n",
    "    edges = [(vnum[k], vnum[val]) for k in list(net.keys()) for val in net[k]]\n",
    "    G = ig.Graph(N, directed = False)\n",
    "    G.add_edges(edges)\n",
    "    G.vs[\"label\"] = list_name\n",
    "    G.simplify()\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610eb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vértices:  120 \n",
      "Arestas:  282\n"
     ]
    }
   ],
   "source": [
    "g = network(net1)\n",
    "print('vértices: ', g.vcount(), '\\nArestas: ', g.ecount())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922630a8",
   "metadata": {},
   "source": [
    "Propriedades estatísticas da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f4a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = dict(collections.Counter(g.degree()))\n",
    "\n",
    "gig = g.clusters().giant()\n",
    "avg_dist = gig.average_path_length(directed=False)\n",
    "size_gig = gig.vcount()\n",
    "diameter = gig.diameter(directed=False)\n",
    "assort = gig.assortativity_degree()\n",
    "coef_agg = sum([k for k in g.transitivity_local_undirected() if k > 0])/g.vcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46684c6b",
   "metadata": {},
   "source": [
    "Calculo das comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5154aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.write_graphml('graph_colaboracao_INCT.graphML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf45453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx\n",
    "gnx = nx.read_graphml('graph_colaboracao_INCT.graphML')\n",
    "\n",
    "#maior aglomerado\n",
    "Gcc = sorted(nx.connected_components(gnx), key=len, reverse=True)\n",
    "gtnx = gnx.subgraph(Gcc[0])\n",
    "communit = nx_comm.louvain_communities(gtnx, resolution=1, seed=None)\n",
    "\n",
    "print('Número de comunidades: ', len(communit))\n",
    "print('\\n')\n",
    "print(communit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fcf63a",
   "metadata": {},
   "source": [
    "# Rede da interação entre as cidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4efd1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados geograficos para vizualizar a rede no Gephi no layout \"Map of Countries\" e #Geo Layout#\n",
    "latitude = {}\n",
    "longitude = {}\n",
    "\n",
    "latitude['Arapiraca'], longitude['Arapiraca'] = -9.75164, -36.6604\n",
    "latitude['Araras'], longitude['Araras'] = -22.3577, -47.3849\n",
    "latitude['Baurú'], longitude['Baurú'] = -22.3154, -49.0615\n",
    "latitude['Belo Horizonte'], longitude['Belo Horizonte'] = -19.8157, -43.9542\n",
    "latitude['Campinas'], longitude['Campinas'] = -22.9064, -47.0616\n",
    "latitude['Cornélio Procópio'], longitude['Cornélio Procópio'] = -23.1813, -50.648\n",
    "latitude['Cuiabá'], longitude['Cuiabá'] = -15.5989, -56.0949\n",
    "latitude['Curitiba'], longitude['Curitiba'] = -25.4284, -49.2733\n",
    "latitude['Florianópolis'], longitude['Florianópolis'] = -27.5969, -48.5495\n",
    "latitude['Goiânia'], longitude['Goiânia'] = -16.6799, -49.255\n",
    "latitude['Itapeva'], longitude['Itapeva'] = -23.9676, -48.901\n",
    "latitude['Juiz de Fora'], longitude['Juiz de Fora'] = -21.7642, -43.3496\n",
    "latitude['Maceió'], longitude['Maceió'] = -9.66625, -35.7351\n",
    "latitude['Natal'], longitude['Natal'] = -5.79448, -35.211\n",
    "latitude['Niteroi'], longitude['Niteroi'] = -22.8808, -43.1043\n",
    "latitude['Patrocínio'], longitude['Patrocínio'] = -18.9441, -46.9924\n",
    "latitude['Petrópolis'], longitude['Petrópolis'] = -22.5046, -43.1823\n",
    "latitude['Ponta Grossa'], longitude['Ponta Grossa'] = -25.0945, -50.1633\n",
    "latitude['Recife'], longitude['Recife'] = -8.05428, -34.8813\n",
    "latitude['Rio de Janeiro'], longitude['Rio de Janeiro'] = -22.9035, -43.2096\n",
    "latitude['Santa Maria'], longitude['Santa Maria'] = -29.6914, -53.8008\n",
    "latitude['Santo André'], longitude['Santo André'] = -23.6666, -46.5322\n",
    "latitude['Sorocaba'], longitude['Sorocaba'] = -23.5062, -47.4559\n",
    "latitude['São Carlos'], longitude['São Carlos'] = -22.0154, -47.8911\n",
    "latitude['São Paulo'], longitude['São Paulo'] = -23.5489, -46.6388\n",
    "latitude['Uberlândia'], longitude['Uberlândia'] = -18.9113, -48.2622\n",
    "latitude['Vitória'], longitude['Vitória'] = -20.3222, -40.3381\n",
    "latitude['Viçosa'], longitude['Viçosa'] = -20.7546, -42.8825\n",
    "latitude['Volta Redonda'], longitude['Volta Redonda'] = -22.5252, -44.1038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db7974c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abrev_names = {}\n",
    "abrev_names_inv = {}\n",
    "for key, name in zip(df2.Abrev, df2.Nome):\n",
    "    abrev_names[key] = name\n",
    "    abrev_names_inv[name] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "29d06231",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_au = {}\n",
    "for key in list(abrev_names_inv.keys()):\n",
    "    city_au[abrev_names_inv[key]] = dcity[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e30f61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_graph(net, city_au, latitude, longitude):\n",
    "    list_name = sorted(set(list(fcity.keys())))\n",
    "    vnum = {v:i for i,v in enumerate(list_name)}\n",
    "    numv = {i:v for i,v in enumerate(list_name)}\n",
    "    N = len(list_name)\n",
    "    \n",
    "    lt = [latitude[k] for k in list_name]\n",
    "    lg = [longitude[k] for k in list_name]\n",
    "\n",
    "    edges = [(vnum[city_au[k]], vnum[city_au[val]]) for k in list(net.keys()) for val in net[k] if val<k]\n",
    "    G = ig.Graph(N, directed = False)\n",
    "    G.add_edges(edges)\n",
    "    G.vs[\"label\"] = list_name\n",
    "    G.vs[\"city\"] = list_name\n",
    "    G.vs[\"lat\"] = lt\n",
    "    G.vs[\"lng\"] = lg\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "087ef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cities = geo_graph(net1, city_au, latitude, longitude)\n",
    "#Vizualização no Gephi com permisão de auto-loops e Estrategia de mesclagem para arestas = Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c64c4a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vértices:  29 \n",
      "Arestas:  282\n"
     ]
    }
   ],
   "source": [
    "print('vértices: ', g_cities.vcount(), '\\nArestas: ', g_cities.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fb36fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cities.write_graphml('graph_colaboracao_cidades_INCT.graphML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbe148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
